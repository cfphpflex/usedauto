{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMYfcKeDY85K"
      },
      "source": [
        "## **Getting started**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3wE0J4j_l9"
      },
      "source": [
        "# Used Car Price Prediction Model\n",
        "\n",
        "## Overview\n",
        "This Used Car Price Prediction Model implements basic CRISP-DM pipeline and additional advanced techniques, down to regional price adjustment. The model has sophisticated feature engineering, proper data preprocessing, and robust prediction pipelines. Additional advanced ML functioality in training and creating model relies on GridSearch, RandomForest,  ...\n",
        "\n",
        "##  Architecture\n",
        "\n",
        "### 1. Data Preprocessing Pipeline\n",
        "I use a comprehensive preprocessing pipeline implemented in the `VehiclePricePredictor` class:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "from joblib import dump as joblib_dump\n",
        "import xgboost as xgb #additional, learning to use my capstone project\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n"
      ],
      "metadata": {
        "id": "HVl8DUy-3Yx5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6l_jGaAef6Q",
        "outputId": "10c6c91e-f1d7-4773-9bc4-4391206c1712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'dataset_statistics.txt', 'vehicles.csv.zip', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "#file list\n",
        "files = os.listdir('/content/')\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DP1O5Kue1ms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1fdade8b-8dbe-4ae2-f943-7e1f7d4cf5ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ea6f15f-a3ab-4bf1-8f77-2efb58431f56\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ea6f15f-a3ab-4bf1-8f77-2efb58431f56\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vehicles.csv.zip to vehicles.csv.zip\n"
          ]
        }
      ],
      "source": [
        "# upload data csv files, uncomment\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NeXkrEx8egmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e81a8a-fd87-4a57-b798-5150f2ec2312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in ZIP: ['vehicles.csv', '__MACOSX/._vehicles.csv']\n"
          ]
        }
      ],
      "source": [
        "#load &  unzip uploaded csv file\n",
        "# cardata = pd.read_csv('/content/vehicles.csv.zip')\n",
        "# print(cpn.count())\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "zip_path = \"/content/vehicles.csv.zip\"  # Update with your ZIP path\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    file_list = zip_ref.namelist()\n",
        "    print(\"Files in ZIP:\", file_list)  # Optional: to view file names\n",
        "    with zip_ref.open(\"vehicles.csv\") as f:  # Use correct filename here\n",
        "        cardata = pd.read_csv(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "CwK57fjCYkEV",
        "outputId": "bd323450-9ed8-40d8-8b65-fa6d18bef6b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id                                                url  \\\n",
              "0  7222695916  https://prescott.craigslist.org/cto/d/prescott...   \n",
              "1  7218891961  https://fayar.craigslist.org/ctd/d/bentonville...   \n",
              "2  7221797935  https://keys.craigslist.org/cto/d/summerland-k...   \n",
              "3  7222270760  https://worcester.craigslist.org/cto/d/west-br...   \n",
              "4  7210384030  https://greensboro.craigslist.org/cto/d/trinit...   \n",
              "\n",
              "                   region                         region_url  price  year  \\\n",
              "0                prescott    https://prescott.craigslist.org   6000   NaN   \n",
              "1            fayetteville       https://fayar.craigslist.org  11900   NaN   \n",
              "2            florida keys        https://keys.craigslist.org  21000   NaN   \n",
              "3  worcester / central MA   https://worcester.craigslist.org   1500   NaN   \n",
              "4              greensboro  https://greensboro.craigslist.org   4900   NaN   \n",
              "\n",
              "  manufacturer model condition cylinders  ... size  type paint_color  \\\n",
              "0          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
              "1          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
              "2          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
              "3          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
              "4          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
              "\n",
              "  image_url description county state lat long posting_date  \n",
              "0       NaN         NaN    NaN    az NaN  NaN          NaN  \n",
              "1       NaN         NaN    NaN    ar NaN  NaN          NaN  \n",
              "2       NaN         NaN    NaN    fl NaN  NaN          NaN  \n",
              "3       NaN         NaN    NaN    ma NaN  NaN          NaN  \n",
              "4       NaN         NaN    NaN    nc NaN  NaN          NaN  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67b897dd-5205-4a79-a99e-b36d153a8afe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>region</th>\n",
              "      <th>region_url</th>\n",
              "      <th>price</th>\n",
              "      <th>year</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>model</th>\n",
              "      <th>condition</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>...</th>\n",
              "      <th>size</th>\n",
              "      <th>type</th>\n",
              "      <th>paint_color</th>\n",
              "      <th>image_url</th>\n",
              "      <th>description</th>\n",
              "      <th>county</th>\n",
              "      <th>state</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>posting_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7222695916</td>\n",
              "      <td>https://prescott.craigslist.org/cto/d/prescott...</td>\n",
              "      <td>prescott</td>\n",
              "      <td>https://prescott.craigslist.org</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>az</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7218891961</td>\n",
              "      <td>https://fayar.craigslist.org/ctd/d/bentonville...</td>\n",
              "      <td>fayetteville</td>\n",
              "      <td>https://fayar.craigslist.org</td>\n",
              "      <td>11900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ar</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7221797935</td>\n",
              "      <td>https://keys.craigslist.org/cto/d/summerland-k...</td>\n",
              "      <td>florida keys</td>\n",
              "      <td>https://keys.craigslist.org</td>\n",
              "      <td>21000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>fl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7222270760</td>\n",
              "      <td>https://worcester.craigslist.org/cto/d/west-br...</td>\n",
              "      <td>worcester / central MA</td>\n",
              "      <td>https://worcester.craigslist.org</td>\n",
              "      <td>1500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ma</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7210384030</td>\n",
              "      <td>https://greensboro.craigslist.org/cto/d/trinit...</td>\n",
              "      <td>greensboro</td>\n",
              "      <td>https://greensboro.craigslist.org</td>\n",
              "      <td>4900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67b897dd-5205-4a79-a99e-b36d153a8afe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67b897dd-5205-4a79-a99e-b36d153a8afe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67b897dd-5205-4a79-a99e-b36d153a8afe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-100410f1-c597-4540-a7f0-b358313a578f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-100410f1-c597-4540-a7f0-b358313a578f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-100410f1-c597-4540-a7f0-b358313a578f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Explore data\n",
        "cardata.head()\n",
        "# cpn.describe()\n",
        "# cpn.info()\n",
        "# cpn.isna().sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "alYrAyM6edEw"
      },
      "outputs": [],
      "source": [
        "def load_and_explore_data():\n",
        "    \"\"\"load dataset w/ filtering\"\"\"\n",
        "    print(\"Loading complete dataset...\")\n",
        "    # df = pd.read_csv('../data/vehicles.csv')\n",
        "\n",
        "    import zipfile\n",
        "    import pandas as pd\n",
        "\n",
        "    zip_path = \"/content/vehicles.csv.zip\"  # Update with your ZIP path\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(\"Files in ZIP:\", file_list)  # Optional: to view file names\n",
        "        with zip_ref.open(\"vehicles.csv\") as f:  # Use correct filename here\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "\n",
        "    print(f\"\\nInitial records: {len(df)}\")\n",
        "\n",
        "    # Keep only relevant columns\n",
        "    columns_to_keep = ['year', 'manufacturer', 'model', 'condition', 'odometer',\n",
        "                      'fuel', 'title_status', 'transmission', 'drive', 'type',\n",
        "                      'paint_color', 'price']\n",
        "    df = df[columns_to_keep]\n",
        "\n",
        "    # Convert categorical columns to lowercase\n",
        "    categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                       'transmission', 'drive', 'type', 'paint_color']\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].str.lower()\n",
        "\n",
        "    # Initial cleaning\n",
        "    df = df.dropna(subset=['price', 'manufacturer', 'model', 'year'])\n",
        "    print(f\"Records after dropping missing values: {len(df)}\")\n",
        "\n",
        "    # Remove unrealistic prices\n",
        "    df = df[\n",
        "        (df['price'] > 5000) &  # Remove very low prices\n",
        "        (df['price'] < 30000)  # Remove extremely high prices\n",
        "    ]\n",
        "    print(f\"Records after price filtering: {len(df)}\")\n",
        "\n",
        "    # Remove outliers using IQR method for price\n",
        "    Q1 = df['price'].quantile(0.25)\n",
        "    Q3 = df['price'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[\n",
        "        (df['price'] >= Q1 - 1.5 * IQR) &\n",
        "        (df['price'] <= Q3 + 1.5 * IQR)\n",
        "    ]\n",
        "    print(f\"Records after removing price outliers: {len(df)}\")\n",
        "\n",
        "    # Year filtering\n",
        "    current_year = 2024\n",
        "    df = df[\n",
        "        (df['year'] >= 1990) &  # Remove very old vehicles\n",
        "        (df['year'] <= current_year)  # Remove future years\n",
        "    ]\n",
        "    print(f\"Records after year filtering: {len(df)}\")\n",
        "\n",
        "    # Mileage filtering\n",
        "    df = df[\n",
        "        (df['odometer'] >= 20000) &  # Remove negative mileage\n",
        "        (df['odometer'] <= 150000)  # Remove extreme mileage\n",
        "    ]\n",
        "    print(f\"Records after mileage filtering: {len(df)}\")\n",
        "\n",
        "    # Condition filtering\n",
        "    valid_conditions = ['excellent', 'good', 'fair', 'like new']\n",
        "    df = df[df['condition'].isin(valid_conditions)]\n",
        "    print(f\"Records after condition filtering: {len(df)}\")\n",
        "\n",
        "    # Title status filtering\n",
        "    valid_titles = ['clean', 'rebuilt', 'lien']\n",
        "    df = df[df['title_status'].isin(valid_titles)]\n",
        "    print(f\"Records after title filtering: {len(df)}\")\n",
        "\n",
        "    # Calculate price per mile and remove outliers\n",
        "    df['price_per_mile'] = df['price'] / (df['odometer'] + 1)\n",
        "    Q1_ppm = df['price_per_mile'].quantile(0.25)\n",
        "    Q3_ppm = df['price_per_mile'].quantile(0.75)\n",
        "    IQR_ppm = Q3_ppm - Q1_ppm\n",
        "    df = df[\n",
        "        (df['price_per_mile'] >= Q1_ppm - 1.5 * IQR_ppm) &\n",
        "        (df['price_per_mile'] <= Q3_ppm + 1.5 * IQR_ppm)\n",
        "    ]\n",
        "    df = df.drop('price_per_mile', axis=1)\n",
        "    print(f\"Records after price/mile filtering: {len(df)}\")\n",
        "\n",
        "    # Fill missing values\n",
        "    df['odometer'] = df['odometer'].fillna(df.groupby(['manufacturer', 'model', 'year'])['odometer'].transform('median'))\n",
        "    df['odometer'] = df['odometer'].fillna(df.groupby(['manufacturer', 'year'])['odometer'].transform('median'))\n",
        "    df['odometer'] = df['odometer'].fillna(df['odometer'].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('unknown')\n",
        "\n",
        "    # Print final statistics\n",
        "    print(\"\\nFinal Dataset Statistics:\")\n",
        "    print(\"\\nRecords by manufacturer (top 10):\")\n",
        "    print(df['manufacturer'].value_counts().head(10))\n",
        "    print(\"\\nPrice statistics by manufacturer (top 10):\")\n",
        "    print(df.groupby('manufacturer')['price'].agg(['count', 'mean', 'std', 'min', 'max']).head(10))\n",
        "\n",
        "    # Save detailed statistics\n",
        "    stats_file = 'dataset_statistics.txt'\n",
        "    with open(stats_file, 'w') as f:\n",
        "        f.write(\"Dataset Statistics\\n\")\n",
        "        f.write(\"=================\\n\\n\")\n",
        "        f.write(f\"Total records: {len(df)}\\n\\n\")\n",
        "\n",
        "        f.write(\"Price Statistics by Manufacturer:\\n\")\n",
        "        f.write(df.groupby('manufacturer')['price'].describe().to_string())\n",
        "\n",
        "        f.write(\"\\n\\nPrice Statistics by Year:\\n\")\n",
        "        f.write(df.groupby('year')['price'].describe().to_string())\n",
        "\n",
        "        f.write(\"\\n\\nPrice Statistics by Condition:\\n\")\n",
        "        f.write(df.groupby('condition')['price'].describe().to_string())\n",
        "\n",
        "        f.write(\"\\n\\nMileage Statistics by Year:\\n\")\n",
        "        f.write(df.groupby('year')['odometer'].describe().to_string())\n",
        "\n",
        "    print(f\"\\nDetailed statistics saved to {stats_file}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def prepare_features(self, data, is_training=False):\n",
        "        \"\"\"Prepare features for model training or prediction\"\"\"\n",
        "        try:\n",
        "            # Convert to DataFrame if dict\n",
        "            if isinstance(data, dict):\n",
        "                data = pd.DataFrame([data])\n",
        "\n",
        "            # Convert categorical columns to lowercase\n",
        "            categorical_features = [\n",
        "                'manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                'transmission', 'drive', 'type', 'paint_color'\n",
        "            ]\n",
        "            numeric_features = ['year', 'odometer']\n",
        "\n",
        "            # Fill missing values\n",
        "            for col in numeric_features:\n",
        "                if col not in data.columns:\n",
        "                    data[col] = 50000 if col == 'odometer' else 2020\n",
        "                data[col] = data[col].fillna(50000 if col == 'odometer' else 2020)\n",
        "\n",
        "            for col in categorical_features:\n",
        "                if col in data.columns:\n",
        "                    data[col] = data[col].str.lower() if data[col].dtype == 'object' else data[col]\n",
        "                    data[col] = data[col].fillna('unknown')\n",
        "                else:\n",
        "                    data[col] = 'unknown'\n",
        "\n",
        "            # Calculate derived features\n",
        "            current_year = datetime.now().year\n",
        "            data['age'] = current_year - data['year']\n",
        "            data['miles_per_year'] = data['odometer'] / data['age'].replace(0, 1)  # Avoid division by zero\n",
        "            data['age_mileage'] = data['age'] * data['miles_per_year']\n",
        "\n",
        "            # Create dummy variables for categorical features\n",
        "            dummy_features = pd.get_dummies(data[categorical_features], prefix=categorical_features)\n",
        "\n",
        "            # Combine numeric and dummy features\n",
        "            numeric_data = data[['year', 'odometer', 'age', 'miles_per_year', 'age_mileage']]\n",
        "            features = pd.concat([numeric_data, dummy_features], axis=1)\n",
        "\n",
        "            # Ensure all features are float type\n",
        "            features = features.astype(float)\n",
        "\n",
        "            print(f\"Prepared features shape: {features.shape}\")\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing features: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "p30mXplX9pRR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zl229f3qZkFN"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train an improved XGBoost model with optimized cross-validation\"\"\"\n",
        "    # Define XGBoost parameters\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'mae',\n",
        "        'max_depth': 8,           # Reduced from 12 to prevent overfitting\n",
        "        'learning_rate': 0.05,    # Slower learning rate for better generalization\n",
        "        'n_estimators': 500,      # More trees for better accuracy\n",
        "        'min_child_weight': 5,    # Increased to prevent overfitting\n",
        "        'subsample': 0.8,         # Random subsampling of training data\n",
        "        'colsample_bytree': 0.8,  # Random subsampling of features\n",
        "        'reg_alpha': 0.1,         # L1 regularization\n",
        "        'reg_lambda': 1.0,        # L2 regularization\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    # Create DMatrix for XGBoost\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "    # Perform cross-validation with reduced folds\n",
        "    print(\"Performing cross-validation...\")\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=params['n_estimators'],\n",
        "        nfold=3,  # Reduced from 5 to 3 folds\n",
        "        metrics=['mae'],\n",
        "        early_stopping_rounds=20,  # Reduced from 50 to 20\n",
        "        verbose_eval=10,  # Print progress every 10 rounds\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Get best number of trees from CV\n",
        "    best_n_estimators = cv_results.shape[0]\n",
        "    params['n_estimators'] = best_n_estimators\n",
        "\n",
        "    # Train final model\n",
        "    print(\"Training XGBoost model...\")\n",
        "    model = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=best_n_estimators,\n",
        "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "        verbose_eval=10  # Print progress every 10 rounds\n",
        "    )\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_pred = model.predict(dval)\n",
        "    val_mae = np.mean(np.abs(val_pred - y_val))\n",
        "    val_mape = np.mean(np.abs((val_pred - y_val) / y_val)) * 100\n",
        "\n",
        "    print(f\"Cross-validation MAE: ${cv_results['test-mae-mean'].iloc[-1]:,.2f}\")\n",
        "    print(f\"Validation MAE: ${val_mae:,.2f}\")\n",
        "    print(f\"Validation MAPE: {val_mape:.1f}%\")\n",
        "\n",
        "    # Calculate and print R² score\n",
        "    r2_score = 1 - np.sum((y_val - val_pred) ** 2) / np.sum((y_val - np.mean(y_val)) ** 2)\n",
        "    print(f\"Validation R² Score: {r2_score:.3f}\")\n",
        "\n",
        "    # Calculate feature importance using XGBoost's native method\n",
        "    try:\n",
        "        # Get feature importance scores\n",
        "        importance = model.get_score(importance_type='gain')\n",
        "\n",
        "        # Create DataFrame with feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': list(importance.keys()),\n",
        "            'importance': list(importance.values())\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 most important features:\")\n",
        "        print(feature_importance.head(10).to_string())\n",
        "\n",
        "        # Save feature importance\n",
        "        feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "        print(\"\\nFeature importance saved to feature_importance.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not calculate feature importance: {str(e)}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load(self, directory='models'):\n",
        "        \"\"\"Load the model and artifacts\"\"\"\n",
        "        try:\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "            # Load model\n",
        "            model_path = os.path.join(directory, 'model.joblib')\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "            self.model = joblib.load(model_path)\n",
        "            print(f\"Model loaded from {model_path}\")\n",
        "\n",
        "            # Load scaler\n",
        "            scaler_path = os.path.join(directory, 'scaler.joblib')\n",
        "            if not os.path.exists(scaler_path):\n",
        "                raise FileNotFoundError(f\"Scaler file not found at {scaler_path}\")\n",
        "            self.scaler = joblib.load(scaler_path)\n",
        "            print(f\"Scaler loaded from {scaler_path}\")\n",
        "\n",
        "            # Load feature info\n",
        "            feature_info_path = os.path.join(directory, 'feature_info.json')\n",
        "            if not os.path.exists(feature_info_path):\n",
        "                raise FileNotFoundError(f\"Feature info file not found at {feature_info_path}\")\n",
        "            with open(feature_info_path, 'r') as f:\n",
        "                self.feature_info = json.load(f)\n",
        "            print(f\"Feature info loaded from {feature_info_path}\")\n",
        "\n",
        "            # Extract feature names and category mappings\n",
        "            self.feature_names = self.feature_info['feature_columns']\n",
        "            self.category_mappings = self.feature_info.get('category_mappings', {})\n",
        "\n",
        "            print(\"Model and artifacts loaded successfully!\")\n",
        "            return self.model, self.scaler, self.feature_info\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model and artifacts: {str(e)}\")\n",
        "            return None, None, None\n"
      ],
      "metadata": {
        "id": "9zcWNK5I9u9-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_kgJF3xmg7vM"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train a RandomForest model.\"\"\"\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    val_score = np.mean(np.abs(model.predict(X_val) - y_val))\n",
        "    print(f\"Validation MAE: {val_score:.2f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g_YBOh2hgghz"
      },
      "outputs": [],
      "source": [
        "def analyze_manufacturer_accuracy(model, X_val, y_val, df_val):\n",
        "    \"\"\"Analyze prediction accuracy by manufacturer\"\"\"\n",
        "    # Get predictions using XGBoost's predict method\n",
        "    dval = xgb.DMatrix(X_val)\n",
        "    predictions = model.predict(dval)\n",
        "\n",
        "    # Create a DataFrame with validation data\n",
        "    val_df = pd.DataFrame({\n",
        "        'manufacturer': df_val['manufacturer'].values,\n",
        "        'price': y_val.values,\n",
        "        'prediction': predictions\n",
        "    })\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_df['error'] = np.abs(val_df['prediction'] - val_df['price'])\n",
        "    val_df['error_pct'] = (val_df['error'] / val_df['price']) * 100\n",
        "\n",
        "    # Group by manufacturer\n",
        "    accuracy_by_make = val_df.groupby('manufacturer').agg({\n",
        "        'error': 'mean',\n",
        "        'error_pct': 'mean',\n",
        "        'price': 'count'\n",
        "    }).sort_values('error_pct')\n",
        "\n",
        "    # Rename columns\n",
        "    accuracy_by_make.columns = ['MAE', 'MAPE', 'Sample_Size']\n",
        "\n",
        "    # Format results\n",
        "    accuracy_by_make['MAE'] = accuracy_by_make['MAE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "    accuracy_by_make['MAPE'] = accuracy_by_make['MAPE'].apply(lambda x: f\"{x:.1f}%\")\n",
        "\n",
        "    print(\"\\nPrediction Accuracy by Manufacturer (Top 10):\")\n",
        "    print(accuracy_by_make.head(10).to_string())\n",
        "\n",
        "    return accuracy_by_make\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mileage_impact(model, X_val, y_val, df_val):\n",
        "    \"\"\"Analyze how mileage affects prediction accuracy with more granular brackets\"\"\"\n",
        "    # Get predictions using XGBoost's predict method\n",
        "    dval = xgb.DMatrix(X_val)\n",
        "    predictions = model.predict(dval)\n",
        "\n",
        "    # Create a DataFrame with validation data\n",
        "    val_df = pd.DataFrame({\n",
        "        'odometer': df_val['odometer'].values,\n",
        "        'price': y_val.values,\n",
        "        'prediction': predictions\n",
        "    })\n",
        "\n",
        "    # Calculate error metrics\n",
        "    val_df['error'] = np.abs(val_df['prediction'] - val_df['price'])\n",
        "    val_df['error_pct'] = (val_df['error'] / val_df['price']) * 100\n",
        "\n",
        "    # Create more granular mileage bins\n",
        "    bins = [0, 10000, 25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000, float('inf')]\n",
        "    labels = ['0-10k', '10k-25k', '25k-50k', '50k-75k', '75k-100k',\n",
        "              '100k-125k', '125k-150k', '150k-175k', '175k-200k', '200k+']\n",
        "    val_df['mileage_bin'] = pd.cut(val_df['odometer'], bins=bins, labels=labels)\n",
        "\n",
        "    # Group by mileage bin\n",
        "    accuracy_by_mileage = val_df.groupby('mileage_bin').agg({\n",
        "        'error': 'mean',\n",
        "        'error_pct': 'mean',\n",
        "        'price': ['count', 'mean', 'std']\n",
        "    }).sort_values(('error_pct', 'mean'))\n",
        "\n",
        "    # Rename columns\n",
        "    accuracy_by_mileage.columns = ['MAE', 'MAPE', 'Sample_Size', 'Avg_Price', 'Price_Std']\n",
        "\n",
        "    # Format results\n",
        "    accuracy_by_mileage['MAE'] = accuracy_by_mileage['MAE'].apply(lambda x: f\"${x:,.2f}\")\n",
        "    accuracy_by_mileage['MAPE'] = accuracy_by_mileage['MAPE'].apply(lambda x: f\"{x:.1f}%\")\n",
        "    accuracy_by_mileage['Avg_Price'] = accuracy_by_mileage['Avg_Price'].apply(lambda x: f\"${x:,.2f}\")\n",
        "    accuracy_by_mileage['Price_Std'] = accuracy_by_mileage['Price_Std'].apply(lambda x: f\"${x:,.2f}\")\n",
        "\n",
        "    print(\"\\nPrediction Accuracy by Mileage Range (Detailed):\")\n",
        "    print(accuracy_by_mileage.to_string())\n",
        "\n",
        "    return accuracy_by_mileage\n"
      ],
      "metadata": {
        "id": "4kLu2i0iAuVz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehiclePricePredictor:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_info = None\n",
        "        self.feature_names = None\n",
        "        self.category_mappings = {}\n",
        "\n",
        "    def clean_data(self, df, is_training=True):\n",
        "        \"\"\"Clean the dataset\"\"\"\n",
        "        # Drop rows with missing prices during training\n",
        "        if is_training:\n",
        "            df = df.dropna(subset=['price'])\n",
        "            # Filter out unreasonable prices\n",
        "            df = df[(df['price'] >= 100) & (df['price'] <= 100000)]\n",
        "\n",
        "        # Fill missing values\n",
        "        df['odometer'] = df['odometer'].fillna(df['odometer'].mean() if is_training else 50000)\n",
        "        df['year'] = df['year'].fillna(df['year'].median() if is_training else 2020)\n",
        "\n",
        "        # Fill missing categorical values with 'unknown'\n",
        "        categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                           'transmission', 'drive', 'type', 'paint_color']\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('unknown')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df, is_training=True):\n",
        "        \"\"\"Engineer additional features\"\"\"\n",
        "        # Convert categorical columns to lowercase\n",
        "        for col in ['manufacturer', 'model', 'condition', 'fuel', 'title_status', 'transmission', 'drive', 'type', 'paint_color']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].str.lower()\n",
        "\n",
        "        # Calculate age\n",
        "        df['age'] = 2024 - df['year']\n",
        "\n",
        "        # Calculate mileage per year\n",
        "        df['miles_per_year'] = df['odometer'] / (df['age'] + 1)\n",
        "\n",
        "        # Only calculate price-related features during training\n",
        "        if is_training:\n",
        "            df['price_per_mile'] = df['price'] / (df['odometer'] + 1)\n",
        "            df['price_per_year'] = df['price'] / (df['age'] + 1)\n",
        "\n",
        "        # Create interaction features\n",
        "        df['age_mileage'] = df['age'] * df['odometer']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df, is_training=True):\n",
        "        \"\"\"Preprocess the data for model training or prediction\"\"\"\n",
        "        # Convert categorical columns to lowercase\n",
        "        categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                           'transmission', 'drive', 'type', 'paint_color']\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].str.lower()\n",
        "\n",
        "        # Calculate interaction features\n",
        "        df['age_mileage'] = df['age'] * df['odometer']\n",
        "\n",
        "        # Get all possible categories for each categorical column\n",
        "        self.category_mappings = {}\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                self.category_mappings[col] = sorted(df[col].unique().tolist())\n",
        "\n",
        "        # Create dummy variables\n",
        "        dummies = pd.get_dummies(df[categorical_cols])\n",
        "\n",
        "        # Combine with numeric features\n",
        "        numeric_features = ['year', 'odometer', 'age', 'miles_per_year', 'age_mileage']\n",
        "        X = pd.concat([df[numeric_features], dummies], axis=1)\n",
        "\n",
        "        # Save feature names\n",
        "        self.feature_names = list(X.columns)\n",
        "\n",
        "        # During training, return features and target\n",
        "        if is_training:\n",
        "            y = df['price']\n",
        "            return X, y\n",
        "\n",
        "        # During prediction, ensure we have all the same features as during training\n",
        "        for col in self.feature_names:\n",
        "            if col not in X.columns:\n",
        "                X[col] = 0\n",
        "\n",
        "        # Select only the columns that were present during training\n",
        "        X = X[self.feature_names]\n",
        "\n",
        "        return X\n",
        "\n",
        "    def train(self, data):\n",
        "        \"\"\"Train the model and save artifacts\"\"\"\n",
        "        try:\n",
        "            print(\"Starting model training...\")\n",
        "\n",
        "            # Clean and engineer features\n",
        "            data = self.clean_data(data, is_training=True)\n",
        "            data = self.engineer_features(data, is_training=True)\n",
        "\n",
        "            # Prepare features\n",
        "            features, target = self.preprocess_data(data, is_training=True)\n",
        "            print(f\"Prepared features with shape: {features.shape}\")\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                features, target, test_size=0.2, random_state=42\n",
        "            )\n",
        "            print(\"Split data into train and validation sets\")\n",
        "\n",
        "            # Scale features\n",
        "            self.scaler = StandardScaler()\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "            X_val_scaled = self.scaler.transform(X_val)\n",
        "            print(\"Scaled features\")\n",
        "\n",
        "            # Train model\n",
        "            self.model = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=None,\n",
        "                min_samples_split=2,\n",
        "                min_samples_leaf=1,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            self.model.fit(X_train_scaled, y_train)\n",
        "            print(\"Trained model\")\n",
        "\n",
        "            # Evaluate model\n",
        "            val_score = np.mean(np.abs(self.model.predict(X_val_scaled) - y_val))\n",
        "            print(f\"Validation MAE: {val_score:.2f}\")\n",
        "\n",
        "            # Save feature information\n",
        "            feature_info = {\n",
        "                'feature_columns': features.columns.tolist(),\n",
        "                'numeric_features': ['year', 'odometer', 'age', 'miles_per_year', 'age_mileage'],\n",
        "                'categorical_features': [\n",
        "                    'manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                    'transmission', 'drive', 'type', 'paint_color'\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            # Save artifacts\n",
        "            os.makedirs('models', exist_ok=True)\n",
        "            joblib_dump(self.model, os.path.join('models', 'model.joblib'))\n",
        "            joblib_dump(self.scaler, os.path.join('models', 'scaler.joblib'))\n",
        "            with open(os.path.join('models', 'feature_info.json'), 'w') as f:\n",
        "                json.dump(feature_info, f, indent=4)\n",
        "            print(\"Saved model artifacts\")\n",
        "\n",
        "            return val_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, vehicle):\n",
        "        \"\"\"Make a price prediction for a single vehicle\"\"\"\n",
        "        # Create DataFrame from vehicle data\n",
        "        df = pd.DataFrame([vehicle])\n",
        "\n",
        "        # Clean and engineer features\n",
        "        df = self.clean_data(df, is_training=False)\n",
        "        df = self.engineer_features(df, is_training=False)\n",
        "\n",
        "        # Preprocess features\n",
        "        X = self.preprocess_data(df, is_training=False)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = self.model.predict(X_scaled)[0]\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def save(self, directory='models'):\n",
        "        \"\"\"Save the model and artifacts\"\"\"\n",
        "        try:\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "            # Save model\n",
        "            model_path = os.path.join(directory, 'model.joblib')\n",
        "            joblib.dump(self.model, model_path)\n",
        "            print(f\"Model saved to {model_path}\")\n",
        "\n",
        "            # Save scaler\n",
        "            scaler_path = os.path.join(directory, 'scaler.joblib')\n",
        "            joblib.dump(self.scaler, scaler_path)\n",
        "            print(f\"Scaler saved to {scaler_path}\")\n",
        "\n",
        "            # Save feature info for training\n",
        "            feature_info_train = {\n",
        "                'feature_columns': self.feature_names,\n",
        "                'numeric_features': ['year', 'odometer', 'age', 'miles_per_year', 'age_mileage'],\n",
        "                'categorical_features': ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                                       'transmission', 'drive', 'type', 'paint_color'],\n",
        "                'category_mappings': self.category_mappings\n",
        "            }\n",
        "            feature_info_train_path = os.path.join(directory, 'feature_info_train.joblib')\n",
        "            joblib.dump(feature_info_train, feature_info_train_path)\n",
        "            print(f\"Training feature info saved to {feature_info_train_path}\")\n",
        "\n",
        "            # Save feature info for prediction\n",
        "            feature_info = {\n",
        "                'feature_names': self.feature_names,\n",
        "                'category_mappings': self.category_mappings\n",
        "            }\n",
        "\n",
        "            # Save as joblib\n",
        "            feature_info_path = os.path.join(directory, 'feature_info.joblib')\n",
        "            joblib.dump(feature_info, feature_info_path)\n",
        "            print(f\"Feature info saved to {feature_info_path}\")\n",
        "\n",
        "            # Save as JSON for readability\n",
        "            feature_info_json_path = os.path.join(directory, 'feature_info.json')\n",
        "            with open(feature_info_json_path, 'w') as f:\n",
        "                json.dump(feature_info, f, indent=4)\n",
        "            print(f\"Feature info saved to {feature_info_json_path}\")\n",
        "\n",
        "            print(\"Model and artifacts saved successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model and artifacts: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def load(self, directory='models'):\n",
        "        \"\"\"Load the model and artifacts\"\"\"\n",
        "        try:\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "            # Load model\n",
        "            model_path = os.path.join(directory, 'model.joblib')\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "            self.model = joblib.load(model_path)\n",
        "            print(f\"Model loaded from {model_path}\")\n",
        "\n",
        "            # Load scaler\n",
        "            scaler_path = os.path.join(directory, 'scaler.joblib')\n",
        "            if not os.path.exists(scaler_path):\n",
        "                raise FileNotFoundError(f\"Scaler file not found at {scaler_path}\")\n",
        "            self.scaler = joblib.load(scaler_path)\n",
        "            print(f\"Scaler loaded from {scaler_path}\")\n",
        "\n",
        "            # Load feature info\n",
        "            feature_info_path = os.path.join(directory, 'feature_info.json')\n",
        "            if not os.path.exists(feature_info_path):\n",
        "                raise FileNotFoundError(f\"Feature info file not found at {feature_info_path}\")\n",
        "            with open(feature_info_path, 'r') as f:\n",
        "                self.feature_info = json.load(f)\n",
        "            print(f\"Feature info loaded from {feature_info_path}\")\n",
        "\n",
        "            # Extract feature names and category mappings\n",
        "            self.feature_names = self.feature_info['feature_columns']\n",
        "            self.category_mappings = self.feature_info.get('category_mappings', {})\n",
        "\n",
        "            print(\"Model and artifacts loaded successfully!\")\n",
        "            return self.model, self.scaler, self.feature_info\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model and artifacts: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def prepare_features(self, data, is_training=False):\n",
        "        \"\"\"Prepare features for model training or prediction\"\"\"\n",
        "        try:\n",
        "            # Convert to DataFrame if dict\n",
        "            if isinstance(data, dict):\n",
        "                data = pd.DataFrame([data])\n",
        "\n",
        "            # Convert categorical columns to lowercase\n",
        "            categorical_features = [\n",
        "                'manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
        "                'transmission', 'drive', 'type', 'paint_color'\n",
        "            ]\n",
        "            numeric_features = ['year', 'odometer']\n",
        "\n",
        "            # Fill missing values\n",
        "            for col in numeric_features:\n",
        "                if col not in data.columns:\n",
        "                    data[col] = 50000 if col == 'odometer' else 2020\n",
        "                data[col] = data[col].fillna(50000 if col == 'odometer' else 2020)\n",
        "\n",
        "            for col in categorical_features:\n",
        "                if col in data.columns:\n",
        "                    data[col] = data[col].str.lower() if data[col].dtype == 'object' else data[col]\n",
        "                    data[col] = data[col].fillna('unknown')\n",
        "                else:\n",
        "                    data[col] = 'unknown'\n",
        "\n",
        "            # Calculate derived features\n",
        "            current_year = datetime.now().year\n",
        "            data['age'] = current_year - data['year']\n",
        "            data['miles_per_year'] = data['odometer'] / data['age'].replace(0, 1)  # Avoid division by zero\n",
        "            data['age_mileage'] = data['age'] * data['miles_per_year']\n",
        "\n",
        "            # Create dummy variables for categorical features\n",
        "            dummy_features = pd.get_dummies(data[categorical_features], prefix=categorical_features)\n",
        "\n",
        "            # Combine numeric and dummy features\n",
        "            numeric_data = data[['year', 'odometer', 'age', 'miles_per_year', 'age_mileage']]\n",
        "            features = pd.concat([numeric_data, dummy_features], axis=1)\n",
        "\n",
        "            # Ensure all features are float type\n",
        "            features = features.astype(float)\n",
        "\n",
        "            print(f\"Prepared features shape: {features.shape}\")\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing features: {str(e)}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "je6iKujI8oFX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_gui_integration():\n",
        "    \"\"\"Test GUI integration and prediction functionality\"\"\"\n",
        "    try:\n",
        "        import tkinter as tk\n",
        "        from gui import VehicleAnalysisGUI\n",
        "\n",
        "        # Create root window\n",
        "        root = tk.Tk()\n",
        "        root.withdraw()  # Hide the main window\n",
        "\n",
        "        # Create predictor instance\n",
        "        predictor = VehiclePricePredictor()\n",
        "        predictor.load()  # Load the saved model and artifacts\n",
        "\n",
        "        # Initialize GUI\n",
        "        gui = VehicleAnalysisGUI(root)\n",
        "\n",
        "        # Test prediction with different vehicles\n",
        "        test_cases = [\n",
        "            {\n",
        "                'year': 2015,\n",
        "                'manufacturer': 'bmw',\n",
        "                'model': '320i',\n",
        "                'condition': 'good',\n",
        "                'odometer': 80000,\n",
        "                'fuel': 'gas',\n",
        "                'title_status': 'clean',\n",
        "                'transmission': 'automatic',\n",
        "                'drive': 'rwd',\n",
        "                'type': 'sedan',\n",
        "                'paint_color': 'white'\n",
        "            },\n",
        "            {\n",
        "                'year': 2018,\n",
        "                'manufacturer': 'toyota',\n",
        "                'model': 'camry',\n",
        "                'condition': 'excellent',\n",
        "                'odometer': 40000,\n",
        "                'fuel': 'gas',\n",
        "                'title_status': 'clean',\n",
        "                'transmission': 'automatic',\n",
        "                'drive': 'fwd',\n",
        "                'type': 'sedan',\n",
        "                'paint_color': 'silver'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        print(\"\\nTesting GUI predictions:\")\n",
        "        for vehicle in test_cases:\n",
        "            # Use the predictor directly\n",
        "            result = predictor.predict(vehicle)\n",
        "            print(f\"\\nVehicle: {vehicle['year']} {vehicle['manufacturer'].upper()} {vehicle['model']}\")\n",
        "            print(f\"Predicted price: ${result['prediction']:,.2f}\")\n",
        "            print(f\"Price range: ${result['confidence_low']:,.2f} - ${result['confidence_high']:,.2f}\")\n",
        "\n",
        "        # Clean up\n",
        "        root.destroy()\n",
        "\n",
        "        print(\"\\nGUI test completed successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"GUI test failed: {str(e)}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "vNyVdqXoAv2c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSLxGradqVix",
        "outputId": "5a98f1c9-c7db-44d4-d846-a3a9abe09d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load and prep data...\n",
            "Loading complete dataset...\n",
            "Files in ZIP: ['vehicles.csv', '__MACOSX/._vehicles.csv']\n",
            "\n",
            "Initial records: 426880\n",
            "Records after dropping missing values: 404020\n",
            "Records after price filtering: 239460\n",
            "Records after removing price outliers: 239460\n",
            "Records after year filtering: 233815\n",
            "Records after mileage filtering: 175400\n",
            "Records after condition filtering: 105987\n",
            "Records after title filtering: 102264\n",
            "Records after price/mile filtering: 95671\n",
            "\n",
            "Final Dataset Statistics:\n",
            "\n",
            "Records by manufacturer (top 10):\n",
            "manufacturer\n",
            "ford          13993\n",
            "chevrolet     11070\n",
            "toyota         7677\n",
            "honda          5682\n",
            "nissan         5669\n",
            "bmw            4649\n",
            "jeep           4268\n",
            "volkswagen     3461\n",
            "hyundai        3283\n",
            "dodge          2972\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Price statistics by manufacturer (top 10):\n",
            "              count          mean          std    min    max\n",
            "manufacturer                                                \n",
            "acura          1671  16909.197487  6138.423698   5200  29990\n",
            "alfa-romeo       79  27166.088608  3893.064620   5800  29590\n",
            "audi           2008  17589.994522  7136.119326   5100  29999\n",
            "bmw            4649  16566.124113  6849.715707   5190  29998\n",
            "buick          1232  13881.399351  6370.346527   5100  29991\n",
            "cadillac       1611  15702.613284  6793.806166   5100  29995\n",
            "chevrolet     11070  15561.780939  7061.903005   5099  29999\n",
            "chrysler       1723  12543.432966  5858.231428   5190  29850\n",
            "dodge          2972  13822.069987  6588.309383   5100  29999\n",
            "ferrari           1  14000.000000          NaN  14000  14000\n",
            "\n",
            "Detailed statistics saved to dataset_statistics.txt\n",
            "Engineering features...\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to train and save the model.\"\"\"\n",
        "    print(\"load and prep data...\")\n",
        "    df = load_and_explore_data()\n",
        "\n",
        "    # Create an instance of VehiclePricePredictor\n",
        "    predictor = VehiclePricePredictor()\n",
        "\n",
        "    print(\"Engineering features...\")\n",
        "    X, y, feature_info = predictor.prepare_features(data=df, is_training=True)\n",
        "\n",
        "    print(\"Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"standardized: Scaling features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model = train_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "    print(\"Saving model and artifacts...\")\n",
        "    save_model(model, scaler, feature_info)\n",
        "\n",
        "    # Test prediction\n",
        "    test_vehicle = {\n",
        "        'year': 2020,\n",
        "        'manufacturer': 'tesla',\n",
        "        'model': 'model 3',\n",
        "        'condition': 'excellent',\n",
        "        'odometer': 25000,\n",
        "        'fuel': 'electric',\n",
        "        'title_status': 'clean',\n",
        "        'transmission': 'automatic',\n",
        "        'drive': 'rwd',\n",
        "        'type': 'sedan',\n",
        "        'paint_color': 'red'\n",
        "    }\n",
        "\n",
        "    X_pred = predictor.prepare_features(data=test_vehicle, is_training=False)\n",
        "    X_pred_scaled = scaler.transform(X_pred)\n",
        "    prediction = model.predict(X_pred_scaled)[0]\n",
        "\n",
        "    print(f\"\\nPredicted price for test vehicle: ${prediction:,.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}