{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMYfcKeDY85K"
   },
   "source": [
    "## **Getting started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ijg5wUCTQYG"
   },
   "source": [
    "**Business Objective:**\n",
    "\n",
    "goal is to understand what factors make a car more or less expensive. As a result of your analysis, you should provide clear recommendations to your client—a used car dealership—as to what consumers value in a used car.\n",
    "\n",
    "**Problem Statement:** Determine which features are most strongly associated with what makes a car more or less expensive.\n",
    "\n",
    "\n",
    "**Proposed Process Resolution:**\n",
    "This notebook analyzes the factors influencing coupon acceptance (Y=1) versus non-acceptance (Y=0):\n",
    "Coupon Acceptance/Rejection Fetures Discovery/Analysis/Correlation. Including data clensing, correlation analysis, and visualizations that compares key features.\n",
    "\n",
    "\n",
    "**Findings:**\n",
    "  **miles** and certain time thresholds (e.g., **toCoupon_GEQ15min** and **fuel**) are strong predictors of coupon acceptance.\n",
    "  **make mdoel, like **temperature**, are more related to coupon rejection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3-rSrWPkbuNh",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "# from google.colab import files\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN3wE0J4j_l9"
   },
   "source": [
    "# Advanced Vehicle Price Prediction Model\n",
    "\n",
    "## Overview\n",
    "This project implements an advanced machine learning model for predicting used vehicle prices using LightGBM (Light Gradient Boosting Machine). The model incorporates sophisticated feature engineering, proper data preprocessing, and robust prediction pipelines.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "### 1. Data Preprocessing Pipeline\n",
    "The model uses a comprehensive preprocessing pipeline implemented in the `VehiclePricePredictor` class:\n",
    "\n",
    "```python\n",
    "class VehiclePricePredictor:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_info = None\n",
    "        self.feature_names = None\n",
    "        self.category_mappings = {}\n",
    "```\n",
    "\n",
    "#### Key Preprocessing Steps:\n",
    "- **Data Cleaning** (`clean_data()`):\n",
    "  - Handles missing values\n",
    "  - Removes outliers (prices < $100 or > $100,000)\n",
    "  - Fills missing odometer readings with mean values\n",
    "  - Fills missing years with median values\n",
    "  - Handles missing categorical values with 'unknown'\n",
    "\n",
    "- **Feature Engineering** (`engineer_features()`):\n",
    "  - Vehicle age calculation (2024 - year)\n",
    "  - Miles per year calculation\n",
    "  - Price per mile (training only)\n",
    "  - Price per year (training only)\n",
    "  - Age-mileage interaction features\n",
    "\n",
    "- **Data Preprocessing** (`preprocess_data()`):\n",
    "  - Categorical variable standardization\n",
    "  - One-hot encoding\n",
    "  - Feature scaling\n",
    "  - Feature name preservation\n",
    "\n",
    "### 2. Model Training\n",
    "The model uses LightGBM with optimized parameters:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "```\n",
    "\n",
    "#### Training Process:\n",
    "1. Data loading and cleaning\n",
    "2. Feature engineering\n",
    "3. Train/validation split\n",
    "4. Model training with early stopping\n",
    "5. Model evaluation using MAE and R² metrics\n",
    "6. Artifact saving (model, scaler, feature info)\n",
    "\n",
    "### 3. Prediction Pipeline\n",
    "The prediction process follows these steps:\n",
    "1. Input data validation\n",
    "2. Feature preprocessing\n",
    "3. Feature engineering\n",
    "4. Model prediction\n",
    "5. Price output\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. Robust Feature Engineering\n",
    "- **Temporal Features**:\n",
    "  - Vehicle age\n",
    "  - Miles per year\n",
    "  - Price per year (training)\n",
    "  - Price per mile (training)\n",
    "\n",
    "- **Interaction Features**:\n",
    "  - Age-mileage interaction\n",
    "  - Manufacturer-model combinations\n",
    "\n",
    "### 2. Categorical Variable Handling\n",
    "- Standardized lowercase conversion\n",
    "- One-hot encoding\n",
    "- Category mapping preservation\n",
    "- Unknown category handling\n",
    "\n",
    "### 3. Model Persistence\n",
    "The model saves three key artifacts:\n",
    "1. `model.joblib`: The trained LightGBM model\n",
    "2. `scaler.pkl`: The StandardScaler for feature normalization\n",
    "3. `feature_info.json`: Feature names and category mappings\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Training the Model\n",
    "```bash\n",
    "python src/improved_model.py\n",
    "```\n",
    "\n",
    "### Making Predictions\n",
    "```bash\n",
    "python src/predict.py\n",
    "```\n",
    "\n",
    "### Using the GUI\n",
    "```bash\n",
    "python src/gui.py\n",
    "```\n",
    "\n",
    "## Performance Metrics\n",
    "The model is evaluated using:\n",
    "- Mean Absolute Error (MAE)\n",
    "- R² Score\n",
    "- Validation set performance\n",
    "- Cross-validation results\n",
    "\n",
    "## Dependencies\n",
    "- pandas\n",
    "- numpy\n",
    "- lightgbm\n",
    "- scikit-learn\n",
    "- joblib\n",
    "- pickle\n",
    "\n",
    "## Future Improvements\n",
    "1. Hyperparameter optimization\n",
    "2. Additional feature engineering\n",
    "3. Ensemble methods\n",
    "4. Real-time price updates\n",
    "5. Market trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6l_jGaAef6Q",
    "outputId": "49aa535c-7ea7-45d8-b638-003c20cb2274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicles.csv.zip', 'archive.zip', 'vehicles.csv']\n"
     ]
    }
   ],
   "source": [
    "#file list\n",
    "files = os.listdir('../data/')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "2DP1O5Kue1ms",
    "outputId": "e11cd162-ddbf-44ce-a687-29b465e829d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-7cfb4ac1-1b5c-4ab5-8b11-94a67fc8c639\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-7cfb4ac1-1b5c-4ab5-8b11-94a67fc8c639\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload data csv files, uncomment\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NeXkrEx8egmR",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              426880\n",
      "url             426880\n",
      "region          426880\n",
      "region_url      426880\n",
      "price           426880\n",
      "year            425675\n",
      "manufacturer    409234\n",
      "model           421603\n",
      "condition       252776\n",
      "cylinders       249202\n",
      "fuel            423867\n",
      "odometer        422480\n",
      "title_status    418638\n",
      "transmission    424324\n",
      "VIN             265838\n",
      "drive           296313\n",
      "size            120519\n",
      "type            334022\n",
      "paint_color     296677\n",
      "image_url       426812\n",
      "description     426810\n",
      "county               0\n",
      "state           426880\n",
      "lat             420331\n",
      "long            420331\n",
      "posting_date    426812\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#load uploaded csv file\n",
    "cardata = pd.read_csv('../data/vehicles.csv')\n",
    "print(cardata.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "CwK57fjCYkEV",
    "outputId": "9513bb31-41d3-4a28-9fe2-9c2b85459fea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>region</th>\n",
       "      <th>region_url</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>condition</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>...</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "      <th>paint_color</th>\n",
       "      <th>image_url</th>\n",
       "      <th>description</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>posting_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7222695916</td>\n",
       "      <td>https://prescott.craigslist.org/cto/d/prescott...</td>\n",
       "      <td>prescott</td>\n",
       "      <td>https://prescott.craigslist.org</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>az</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7218891961</td>\n",
       "      <td>https://fayar.craigslist.org/ctd/d/bentonville...</td>\n",
       "      <td>fayetteville</td>\n",
       "      <td>https://fayar.craigslist.org</td>\n",
       "      <td>11900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7221797935</td>\n",
       "      <td>https://keys.craigslist.org/cto/d/summerland-k...</td>\n",
       "      <td>florida keys</td>\n",
       "      <td>https://keys.craigslist.org</td>\n",
       "      <td>21000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7222270760</td>\n",
       "      <td>https://worcester.craigslist.org/cto/d/west-br...</td>\n",
       "      <td>worcester / central MA</td>\n",
       "      <td>https://worcester.craigslist.org</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7210384030</td>\n",
       "      <td>https://greensboro.craigslist.org/cto/d/trinit...</td>\n",
       "      <td>greensboro</td>\n",
       "      <td>https://greensboro.craigslist.org</td>\n",
       "      <td>4900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  7222695916  https://prescott.craigslist.org/cto/d/prescott...   \n",
       "1  7218891961  https://fayar.craigslist.org/ctd/d/bentonville...   \n",
       "2  7221797935  https://keys.craigslist.org/cto/d/summerland-k...   \n",
       "3  7222270760  https://worcester.craigslist.org/cto/d/west-br...   \n",
       "4  7210384030  https://greensboro.craigslist.org/cto/d/trinit...   \n",
       "\n",
       "                   region                         region_url  price  year  \\\n",
       "0                prescott    https://prescott.craigslist.org   6000   NaN   \n",
       "1            fayetteville       https://fayar.craigslist.org  11900   NaN   \n",
       "2            florida keys        https://keys.craigslist.org  21000   NaN   \n",
       "3  worcester / central MA   https://worcester.craigslist.org   1500   NaN   \n",
       "4              greensboro  https://greensboro.craigslist.org   4900   NaN   \n",
       "\n",
       "  manufacturer model condition cylinders  ... size  type paint_color  \\\n",
       "0          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "1          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "2          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "3          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "4          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "\n",
       "  image_url description county state lat long posting_date  \n",
       "0       NaN         NaN    NaN    az NaN  NaN          NaN  \n",
       "1       NaN         NaN    NaN    ar NaN  NaN          NaN  \n",
       "2       NaN         NaN    NaN    fl NaN  NaN          NaN  \n",
       "3       NaN         NaN    NaN    ma NaN  NaN          NaN  \n",
       "4       NaN         NaN    NaN    nc NaN  NaN          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore data\n",
    "cardata.head()\n",
    "# cpn.describe()\n",
    "# cpn.info()\n",
    "# cpn.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eGsnm8LLc54S"
   },
   "outputs": [],
   "source": [
    "class VehiclePricePredictor:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_info = None\n",
    "        self.feature_names = None\n",
    "        self.category_mappings = {}\n",
    "\n",
    "    def clean_data(self, df, is_training=True):\n",
    "        \"\"\"Clean the dataset\"\"\"\n",
    "        # Drop rows with missing prices during training\n",
    "        if is_training:\n",
    "            df = df.dropna(subset=['price'])\n",
    "            # Filter out unreasonable prices\n",
    "            df = df[(df['price'] >= 100) & (df['price'] <= 100000)]\n",
    "\n",
    "        # Fill missing values\n",
    "        df['odometer'] = df['odometer'].fillna(df['odometer'].mean() if is_training else 50000)\n",
    "        df['year'] = df['year'].fillna(df['year'].median() if is_training else 2020)\n",
    "\n",
    "        # Fill missing categorical values with 'unknown'\n",
    "        categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
    "                           'transmission', 'drive', 'type', 'paint_color']\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('unknown')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df, is_training=True):\n",
    "        \"\"\"Engineer additional features\"\"\"\n",
    "        # Convert categorical columns to lowercase\n",
    "        for col in ['manufacturer', 'model', 'condition', 'fuel', 'title_status', 'transmission', 'drive', 'type', 'paint_color']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].str.lower()\n",
    "\n",
    "        # Calculate age\n",
    "        df['age'] = 2024 - df['year']\n",
    "\n",
    "        # Calculate mileage per year\n",
    "        df['miles_per_year'] = df['odometer'] / (df['age'] + 1)\n",
    "\n",
    "        # Only calculate price-related features during training\n",
    "        if is_training:\n",
    "            df['price_per_mile'] = df['price'] / (df['odometer'] + 1)\n",
    "            df['price_per_year'] = df['price'] / (df['age'] + 1)\n",
    "\n",
    "        # Create interaction features\n",
    "        df['age_mileage'] = df['age'] * df['odometer']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"Preprocess the data for model training or prediction\"\"\"\n",
    "        # Convert categorical columns to lowercase\n",
    "        categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
    "                           'transmission', 'drive', 'type', 'paint_color']\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].str.lower()\n",
    "\n",
    "        # Calculate interaction features\n",
    "        df['age_mileage'] = df['age'] * df['odometer']\n",
    "\n",
    "        # During training, save the feature names and category mappings\n",
    "        if is_training:\n",
    "            # Get all possible categories for each categorical column\n",
    "            self.category_mappings = {}\n",
    "            for col in categorical_cols:\n",
    "                if col in df.columns:\n",
    "                    self.category_mappings[col] = sorted(df[col].unique().tolist())\n",
    "\n",
    "            # Create dummy variables\n",
    "            dummies = pd.get_dummies(df[categorical_cols])\n",
    "\n",
    "            # Combine with numeric features\n",
    "            numeric_features = ['year', 'odometer', 'age', 'miles_per_year', 'age_mileage']\n",
    "            X = pd.concat([df[numeric_features], dummies], axis=1)\n",
    "\n",
    "            # Save feature names\n",
    "            self.feature_names = list(X.columns)\n",
    "\n",
    "            # Return features and target\n",
    "            y = df['price']\n",
    "            return X, y\n",
    "\n",
    "        # During prediction, ensure we have all the same features as during training\n",
    "        dummies = pd.get_dummies(df[categorical_cols])\n",
    "\n",
    "        # Combine with numeric features\n",
    "        numeric_features = ['year', 'odometer', 'age', 'miles_per_year', 'age_mileage']\n",
    "        X = pd.concat([df[numeric_features], dummies], axis=1)\n",
    "\n",
    "        # Add missing columns with zeros\n",
    "        for col in self.feature_names:\n",
    "            if col not in X.columns:\n",
    "                X[col] = 0\n",
    "\n",
    "        # Select only the columns that were present during training\n",
    "        X = X[self.feature_names]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def train(self, data_path, nrows=None):\n",
    "        \"\"\"Train the model with improved feature engineering\"\"\"\n",
    "        print(\"Loading and preparing data...\")\n",
    "        df = pd.read_csv(data_path, nrows=nrows)\n",
    "\n",
    "        print(\"Cleaning data...\")\n",
    "        df = self.clean_data(df)\n",
    "\n",
    "        print(\"Engineering features...\")\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        X, y = self.preprocess_data(df)\n",
    "\n",
    "        # Save feature names and category mappings\n",
    "        self.feature_names = list(X.columns)\n",
    "        print(f\"Number of features: {len(self.feature_names)}\")\n",
    "\n",
    "        # Save feature info immediately after preprocessing\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "\n",
    "        # Save feature names\n",
    "        feature_names_path = os.path.join('models', 'feature_names.txt')\n",
    "        with open(feature_names_path, 'w') as f:\n",
    "            for feature in self.feature_names:\n",
    "                f.write(feature + '\\n')\n",
    "        print(f\"Feature names saved to {feature_names_path}\")\n",
    "\n",
    "        # Save category mappings\n",
    "        category_mappings_path = os.path.join('models', 'category_mappings.txt')\n",
    "        with open(category_mappings_path, 'w') as f:\n",
    "            for col, categories in self.category_mappings.items():\n",
    "                f.write(f\"{col}:{','.join(categories)}\\n\")\n",
    "        print(f\"Category mappings saved to {category_mappings_path}\")\n",
    "\n",
    "        # Split data\n",
    "        print(\"Splitting data...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale features\n",
    "        print(\"Scaling features...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "\n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        self.model = lgb.train(\n",
    "            {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'l1',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': -1\n",
    "            },\n",
    "            lgb.Dataset(X_train_scaled, label=y_train),\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[lgb.Dataset(X_val_scaled, label=y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Save model and artifacts\n",
    "        print(\"Saving model and artifacts...\")\n",
    "        self.save()\n",
    "\n",
    "        # Make a test prediction\n",
    "        test_vehicle = {\n",
    "            'year': 2020,\n",
    "            'manufacturer': 'tesla',\n",
    "            'model': 'model 3',\n",
    "            'condition': 'excellent',\n",
    "            'odometer': 25000,\n",
    "            'fuel': 'electric',\n",
    "            'title_status': 'clean',\n",
    "            'transmission': 'automatic',\n",
    "            'drive': 'rwd',\n",
    "            'type': 'sedan',\n",
    "            'paint_color': 'white'\n",
    "        }\n",
    "        test_df = pd.DataFrame([test_vehicle])\n",
    "        test_df = self.clean_data(test_df, is_training=False)\n",
    "        test_df = self.engineer_features(test_df, is_training=False)\n",
    "        test_X = self.preprocess_data(test_df, is_training=False)\n",
    "        test_X_scaled = self.scaler.transform(test_X)\n",
    "        test_pred = self.model.predict(test_X_scaled)[0]\n",
    "        print(f\"\\nPredicted price for test vehicle: ${test_pred:,.2f}\")\n",
    "\n",
    "    def predict(self, vehicle):\n",
    "        \"\"\"Make a price prediction for a single vehicle\"\"\"\n",
    "        # Create DataFrame from vehicle data\n",
    "        df = pd.DataFrame([vehicle])\n",
    "\n",
    "        # Clean and engineer features\n",
    "        df = self.clean_data(df, is_training=False)\n",
    "        df = self.engineer_features(df, is_training=False)\n",
    "\n",
    "        # Preprocess features\n",
    "        X = self.preprocess_data(df, is_training=False)\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(X_scaled)[0]\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def save(self, directory='models'):\n",
    "        \"\"\"Save the model and artifacts\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "            # Save model using pickle\n",
    "            model_path = os.path.join(directory, 'model.pkl')\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "            # Save scaler using pickle\n",
    "            scaler_path = os.path.join(directory, 'scaler.pkl')\n",
    "            with open(scaler_path, 'wb') as f:\n",
    "                pickle.dump(self.scaler, f)\n",
    "            print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "            # Save feature info with feature names and category mappings\n",
    "            feature_info = {\n",
    "                'feature_columns': self.feature_names,\n",
    "                'category_mappings': self.category_mappings\n",
    "            }\n",
    "\n",
    "            # Print debug info\n",
    "            print(\"\\nFeature info to be saved:\")\n",
    "            print(f\"Number of feature columns: {len(self.feature_names)}\")\n",
    "            print(f\"Number of categories: {len(self.category_mappings)}\")\n",
    "\n",
    "            # Save to temporary file first\n",
    "            feature_info_path = os.path.join(directory, 'feature_info.json')\n",
    "            temp_path = feature_info_path + '.tmp'\n",
    "            with open(temp_path, 'w') as f:\n",
    "                json.dump(feature_info, f, indent=2)\n",
    "\n",
    "            # Verify the temporary file was written correctly\n",
    "            with open(temp_path, 'r') as f:\n",
    "                saved_info = json.load(f)\n",
    "                if not saved_info or 'feature_columns' not in saved_info:\n",
    "                    raise ValueError(\"Failed to save feature info correctly\")\n",
    "\n",
    "            # Move temporary file to final location\n",
    "            os.replace(temp_path, feature_info_path)\n",
    "            print(f\"Feature info saved to {feature_info_path}\")\n",
    "\n",
    "            print(f\"\\nAll artifacts saved to {directory}/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving artifacts: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load(self, directory='models'):\n",
    "        \"\"\"Load the model and artifacts\"\"\"\n",
    "        # Load model\n",
    "        self.model = lgb.Booster(model_file=os.path.join(directory, 'model.txt'))\n",
    "\n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(os.path.join(directory, 'scaler.joblib'))\n",
    "\n",
    "        # Load feature info\n",
    "        with open(os.path.join(directory, 'feature_info.json'), 'r') as f:\n",
    "            self.feature_info = json.load(f)\n",
    "\n",
    "        print(\"Model and artifacts loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "alYrAyM6edEw"
   },
   "outputs": [],
   "source": [
    "def load_and_explore_data():\n",
    "    \"\"\"Load and clean the dataset\"\"\"\n",
    "    # Load data with a reasonable limit\n",
    "    df = pd.read_csv('../data/vehicles.csv', nrows=200000)\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    columns_to_keep = ['year', 'manufacturer', 'model', 'condition', 'odometer',\n",
    "                      'fuel', 'title_status', 'transmission', 'drive', 'type',\n",
    "                      'paint_color', 'price']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Basic cleaning\n",
    "    df = df.dropna(subset=['price'])  # Drop rows with missing prices\n",
    "    df = df[(df['price'] >= 100) & (df['price'] <= 100000)]  # Filter price range\n",
    "\n",
    "    # Convert categorical columns to lowercase\n",
    "    categorical_cols = ['manufacturer', 'model', 'condition', 'fuel', 'title_status',\n",
    "                       'transmission', 'drive', 'type', 'paint_color']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.lower()\n",
    "\n",
    "    # Fill missing values\n",
    "    df['odometer'] = df['odometer'].fillna(df['odometer'].mean())\n",
    "    df['year'] = df['year'].fillna(df['year'].median())\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Zl229f3qZkFN"
   },
   "outputs": [],
   "source": [
    "def prepare_features(data, is_training=True):\n",
    "    \"\"\"\n",
    "    Prepare features for model training or prediction.\n",
    "    Args:\n",
    "        data: DataFrame for training or dict/Series for prediction\n",
    "        is_training: bool, whether preparing for training or prediction\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        df = data.copy()\n",
    "        # Drop rows with missing prices for training data\n",
    "        df = df.dropna(subset=['price'])\n",
    "\n",
    "        # Fill missing values\n",
    "        df['odometer'] = df['odometer'].fillna(df['odometer'].mean())\n",
    "        df['year'] = df['year'].fillna(df['year'].median())\n",
    "\n",
    "        numeric_features = ['year', 'odometer']\n",
    "        categorical_features = ['manufacturer', 'model', 'condition', 'fuel',\n",
    "                              'title_status', 'transmission', 'drive', 'type',\n",
    "                              'paint_color']\n",
    "\n",
    "        # Convert categorical columns to lowercase and fill missing values\n",
    "        for col in categorical_features:\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "\n",
    "        # Calculate derived features\n",
    "        current_year = datetime.now().year\n",
    "        df['age'] = current_year - df['year']\n",
    "        df['miles_per_year'] = df['odometer'] / df['age'].clip(lower=0.1)\n",
    "        df['age_mileage'] = df['age'] * df['odometer']\n",
    "\n",
    "        numeric_features.extend(['age', 'miles_per_year', 'age_mileage'])\n",
    "\n",
    "        # Create dummy variables for categorical features\n",
    "        dummies = pd.get_dummies(df[categorical_features], prefix=categorical_features)\n",
    "\n",
    "        # Store category mapping\n",
    "        category_mapping = {}\n",
    "        for col in categorical_features:\n",
    "            category_mapping[col] = df[col].unique().tolist()\n",
    "\n",
    "        # Combine features\n",
    "        features = pd.concat([df[numeric_features], dummies], axis=1)\n",
    "\n",
    "        # Store feature information\n",
    "        feature_info = {\n",
    "            'numeric_features': numeric_features,\n",
    "            'categorical_features': categorical_features,\n",
    "            'category_mapping': category_mapping,\n",
    "            'all_features': features.columns.tolist()\n",
    "        }\n",
    "\n",
    "        return features, df['price'], feature_info\n",
    "\n",
    "    else:\n",
    "        # Handle single vehicle prediction\n",
    "        if isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])\n",
    "        else:\n",
    "            df = pd.DataFrame([data.to_dict()])\n",
    "\n",
    "        # Load feature info\n",
    "        with open('../models/feature_info.json', 'r') as f:\n",
    "            feature_info = json.load(f)\n",
    "\n",
    "        numeric_features = feature_info['numeric_features']\n",
    "        categorical_features = feature_info['categorical_features']\n",
    "        category_mapping = feature_info['category_mapping']\n",
    "        all_features = feature_info['all_features']\n",
    "\n",
    "        # Convert categorical columns to lowercase\n",
    "        for col in categorical_features:\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "\n",
    "        # Calculate derived features\n",
    "        current_year = datetime.now().year\n",
    "        df['age'] = current_year - df['year']\n",
    "        df['miles_per_year'] = df['odometer'] / df['age'].clip(lower=0.1)\n",
    "        df['age_mileage'] = df['age'] * df['odometer']\n",
    "\n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(df[categorical_features], prefix=categorical_features)\n",
    "\n",
    "        # Ensure all training features are present\n",
    "        for feature in all_features:\n",
    "            if feature not in dummies.columns and feature not in numeric_features:\n",
    "                dummies[feature] = 0\n",
    "\n",
    "        # Combine features\n",
    "        features = pd.concat([df[numeric_features], dummies[all_features[len(numeric_features):]]], axis=1)\n",
    "\n",
    "        return features[all_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_kgJF3xmg7vM"
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train a LightGBM model with early stopping.\"\"\"\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'l1',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g_YBOh2hgghz"
   },
   "outputs": [],
   "source": [
    "def save_model(model, scaler, feature_info):\n",
    "    \"\"\"Save the model, scaler, and feature information.\"\"\"\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    model.save_model('../models/model.txt')\n",
    "\n",
    "    # Save scaler\n",
    "    with open('../models/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Save feature information\n",
    "    with open('../models/feature_info.json', 'w') as f:\n",
    "        json.dump(feature_info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "mSLxGradqVix",
    "outputId": "9eadf193-6702-4499-b44e-d9b4202449e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "Splitting data...\n",
      "Scaling features...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def main():\n",
    "\"\"\"Main function to train and save the model.\"\"\"\n",
    "# print(\"Loading and preparing data...\")\n",
    "df = load_and_explore_data()\n",
    "\n",
    "print(\"Engineering features...\")\n",
    "X, y, feature_info = prepare_features(df, is_training=True)\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = train_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"Saving model and artifacts...\")\n",
    "save_model(model, scaler, feature_info)\n",
    "\n",
    "# Test prediction\n",
    "test_vehicle = {\n",
    "    'year': 2014,\n",
    "    'manufacturer': 'BMW',\n",
    "    'model': '320i',\n",
    "    'condition': 'excellent',\n",
    "    'odometer': 112112,\n",
    "    'fuel': 'gas',\n",
    "    'title_status': 'clean',\n",
    "    'transmission': 'automatic',\n",
    "    'drive': 'rwd',\n",
    "    'type': 'sedan',\n",
    "    'paint_color': 'white'\n",
    "}\n",
    "\n",
    "X_pred = prepare_features(test_vehicle, is_training=False)\n",
    "X_pred_scaled = scaler.transform(X_pred)\n",
    "prediction = model.predict(X_pred_scaled)[0]\n",
    "\n",
    "print(f\"\\nPredicted price for test vehicle: ${prediction:,.2f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
